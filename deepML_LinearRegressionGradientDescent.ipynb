{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee011466-fc16-43f7-96dc-b4354250c7dc",
   "metadata": {},
   "source": [
    "# Linear Regression Using Gradient Descent\n",
    "Write a Python function that performs linear regression using gradient descent. The function should take NumPy arrays X (features with a column of ones for the intercept) and y (target) as input, along with learning rate alpha and the number of iterations, and return the coefficients of the linear regression model as a NumPy array. Round your answer to four decimal places. -0.0 is a valid result for rounding a very small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72e64e0-30f5-4a44-a511-11aa7adc88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "791d261d-b94f-46cf-89ac-f0f4f24324ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the variables\n",
    "X = np.array([[1, 1], [1, 2], [1, 3]])\n",
    "y = np.array([1, 2, 3])\n",
    "alpha = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a61abbb1-c083-4a19-9cc1-2f8885b0651a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1107],\n",
       "       [0.9513]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:\n",
    "    \n",
    "    # HIGH LEVEL STEPS:\n",
    "\n",
    "    \n",
    "    # 1. Calculate the hypothesis h = X * theta\n",
    "    # 2. Calculate the loss/error = y - yhat (h - y)\n",
    "    # 3. Calculate the gradient = X^T * loss / m   (m denotes the number of samples)\n",
    "    # 4. Update the parameters theta = theta - alpha * gradient // Theta + alpha * gradient  (this is from my logistic regression implemenation)\n",
    "    \n",
    "    y = y[:, np.newaxis] # reshape the array to be a 3 x 1\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros((n,1))\n",
    "    curr_theta = theta\n",
    "    for i in range(iterations):\n",
    "        # initial hypothesis or value\n",
    "        h = np.dot(X, curr_theta)\n",
    "        # # calculate the loss / error\n",
    "        loss = h - y\n",
    "        # #loss = np.subtract(h, y)\n",
    "        # cost = np.sum(h**2) / (2*m)\n",
    "        # # # calculate the gradient\n",
    "        gradient = np.dot(X.T,loss) / m\n",
    "        # # # update the parameters theta\n",
    "        stepTheta = curr_theta - alpha * gradient\n",
    "        # update theta\n",
    "        curr_theta = stepTheta\n",
    "    \n",
    "    return np.round(curr_theta, decimals=4)\n",
    "\n",
    "linear_regression_gradient_descent(X, y, alpha, iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
